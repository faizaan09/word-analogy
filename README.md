# word-analogy
Coded NCE loss and Binary Cross Entropy loss to create word embeddings.

Use those word embeddings to perform word analogy

Python Version: 3.6.4

Baseline model Accuracy: 
 1. nce : 
 2. cross entropy loss:

Best Accuracy achieved:
 1. nce : 
 2. cross entropy loss: 


Implementation details:

a. Batch generation:

We consider a particular window size (2 * skip_window + 1) that consists all the context words